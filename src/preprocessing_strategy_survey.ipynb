{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('spacy_env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b4f7acd162c9df97217a038553e0250428649633e42de1e1b77386e9f496cd34"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA, DEP, LEMMA, LOWER, IS_PUNCT, IS_DIGIT, IS_SPACE, IS_STOP\n",
    "import numpy as np\n",
    "import en_core_web_sm\n",
    "import wordninja # for splitting tokens lacking whitespace\n",
    "# import jamspell\n",
    "import contextualSpellCheck\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "narratives = pd.read_csv(\"../data/narratives.csv\")\n",
    "intake = narratives.column_name.str.contains('take')\n",
    "narratives = (narratives[intake])[[\"cr_id\", \"column_name\", \"text\"]]\n",
    "narratives = narratives.drop_duplicates()\n",
    "df = narratives[0:5].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'It is reported that the accused officer failed\\nto terminate a motor vehicle pursuit when\\nordered by Sergeant Hernandez\\nIt is reported that the accused officer failed\\nto terminate a motor vehicle pursuit when\\nordered by Sergeant Hernandez\\nIt is reported that the accused officer failed\\nto terminate a motor vehicle pursuit when\\nordered by sergeant Hernandez.\\nIt is reported that the accused officer failed\\nto terminate a motor vehicle pursuit when\\nordered by Sergeant Hernandez'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "test_text = df[\"text\"][5]\n",
    "test_text"
   ]
  },
  {
   "source": [
    "### Strategy 0: Nada\n",
    "NOTES:\n",
    "- Literelly just use spacy to process text imputs.  does pretty good job tagging everything!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\nIt -- it --- PRON --- True --- 7859011591137717335 --- True\n1\nis -- be --- AUX --- True --- 3411606890003347522 --- True\n2\nreported -- report --- VERB --- True --- 11181246799942687462 --- False\n3\nthat -- that --- SCONJ --- True --- 4380130941430378203 --- True\n4\nthe -- the --- DET --- True --- 7425985699627899538 --- True\n5\naccused -- accuse --- VERB --- True --- 8915410849535181575 --- False\n6\nofficer -- officer --- NOUN --- True --- 9228201189916158328 --- False\n7\nfailed -- fail --- VERB --- True --- 4500079622559289248 --- False\n8\n\n -- \n --- SPACE --- True --- 962983613142996970 --- False\n9\nto -- to --- PART --- True --- 3791531372978436496 --- True\n10\nterminate -- terminate --- VERB --- True --- 974796105764162566 --- False\n11\na -- a --- DET --- True --- 11901859001352538922 --- True\n12\nmotor -- motor --- NOUN --- True --- 1640505308719491870 --- False\n13\nvehicle -- vehicle --- NOUN --- True --- 854351138829791262 --- False\n14\npursuit -- pursuit --- NOUN --- True --- 14575054513208559317 --- False\n15\nwhen -- when --- ADV --- True --- 15807309897752499399 --- True\n16\n\n -- \n --- SPACE --- True --- 962983613142996970 --- False\n17\nordered -- order --- VERB --- True --- 18198004002626200087 --- False\n18\nby -- by --- ADP --- True --- 16764210730586636600 --- True\n19\nSergeant -- Sergeant --- PROPN --- True --- 2732174988197022273 --- False\n20\nHernandez -- Hernandez --- PROPN --- True --- 5312260791442479864 --- False\n21\n\n -- \n --- SPACE --- True --- 962983613142996970 --- False\n22\nIt -- it --- PRON --- True --- 7859011591137717335 --- True\n23\nis -- be --- AUX --- True --- 3411606890003347522 --- True\n24\nreported -- report --- VERB --- True --- 11181246799942687462 --- False\n25\nthat -- that --- SCONJ --- True --- 4380130941430378203 --- True\n26\nthe -- the --- DET --- True --- 7425985699627899538 --- True\n27\naccused -- accuse --- VERB --- True --- 8915410849535181575 --- False\n28\nofficer -- officer --- NOUN --- True --- 9228201189916158328 --- False\n29\nfailed -- fail --- VERB --- True --- 4500079622559289248 --- False\n30\n\n -- \n --- SPACE --- True --- 962983613142996970 --- False\n31\nto -- to --- PART --- True --- 3791531372978436496 --- True\n32\nterminate -- terminate --- VERB --- True --- 974796105764162566 --- False\n33\na -- a --- DET --- True --- 11901859001352538922 --- True\n34\nmotor -- motor --- NOUN --- True --- 1640505308719491870 --- False\n35\nvehicle -- vehicle --- NOUN --- True --- 854351138829791262 --- False\n36\npursuit -- pursuit --- NOUN --- True --- 14575054513208559317 --- False\n37\nwhen -- when --- ADV --- True --- 15807309897752499399 --- True\n38\n\n -- \n --- SPACE --- True --- 962983613142996970 --- False\n39\nordered -- order --- VERB --- True --- 18198004002626200087 --- False\n40\nby -- by --- ADP --- True --- 16764210730586636600 --- True\n41\nSergeant -- Sergeant --- PROPN --- True --- 2732174988197022273 --- False\n42\nHernandez -- Hernandez --- PROPN --- True --- 5312260791442479864 --- False\n43\n\n -- \n --- SPACE --- True --- 962983613142996970 --- False\n44\nIt -- it --- PRON --- True --- 7859011591137717335 --- True\n45\nis -- be --- AUX --- True --- 3411606890003347522 --- True\n46\nreported -- report --- VERB --- True --- 11181246799942687462 --- False\n47\nthat -- that --- SCONJ --- True --- 4380130941430378203 --- True\n48\nthe -- the --- DET --- True --- 7425985699627899538 --- True\n49\naccused -- accuse --- VERB --- True --- 8915410849535181575 --- False\n50\nofficer -- officer --- NOUN --- True --- 9228201189916158328 --- False\n51\nfailed -- fail --- VERB --- True --- 4500079622559289248 --- False\n52\n\n -- \n --- SPACE --- True --- 962983613142996970 --- False\n53\nto -- to --- PART --- True --- 3791531372978436496 --- True\n54\nterminate -- terminate --- VERB --- True --- 974796105764162566 --- False\n55\na -- a --- DET --- True --- 11901859001352538922 --- True\n56\nmotor -- motor --- NOUN --- True --- 1640505308719491870 --- False\n57\nvehicle -- vehicle --- NOUN --- True --- 854351138829791262 --- False\n58\npursuit -- pursuit --- NOUN --- True --- 14575054513208559317 --- False\n59\nwhen -- when --- ADV --- True --- 15807309897752499399 --- True\n60\n\n -- \n --- SPACE --- True --- 962983613142996970 --- False\n61\nordered -- order --- VERB --- True --- 18198004002626200087 --- False\n62\nby -- by --- ADP --- True --- 16764210730586636600 --- True\n63\nsergeant -- sergeant --- PROPN --- True --- 10122840340774289451 --- False\n64\nHernandez -- Hernandez --- PROPN --- True --- 5312260791442479864 --- False\n65\n. -- . --- PUNCT --- True --- 12646065887601541794 --- False\n66\n\n -- \n --- SPACE --- True --- 962983613142996970 --- False\n67\nIt -- it --- PRON --- True --- 7859011591137717335 --- True\n68\nis -- be --- AUX --- True --- 3411606890003347522 --- True\n69\nreported -- report --- VERB --- True --- 11181246799942687462 --- False\n70\nthat -- that --- SCONJ --- True --- 4380130941430378203 --- True\n71\nthe -- the --- DET --- True --- 7425985699627899538 --- True\n72\naccused -- accuse --- VERB --- True --- 8915410849535181575 --- False\n73\nofficer -- officer --- NOUN --- True --- 9228201189916158328 --- False\n74\nfailed -- fail --- VERB --- True --- 4500079622559289248 --- False\n75\n\n -- \n --- SPACE --- True --- 962983613142996970 --- False\n76\nto -- to --- PART --- True --- 3791531372978436496 --- True\n77\nterminate -- terminate --- VERB --- True --- 974796105764162566 --- False\n78\na -- a --- DET --- True --- 11901859001352538922 --- True\n79\nmotor -- motor --- NOUN --- True --- 1640505308719491870 --- False\n80\nvehicle -- vehicle --- NOUN --- True --- 854351138829791262 --- False\n81\npursuit -- pursuit --- NOUN --- True --- 14575054513208559317 --- False\n82\nwhen -- when --- ADV --- True --- 15807309897752499399 --- True\n83\n\n -- \n --- SPACE --- True --- 962983613142996970 --- False\n84\nordered -- order --- VERB --- True --- 18198004002626200087 --- False\n85\nby -- by --- ADP --- True --- 16764210730586636600 --- True\n86\nSergeant -- Sergeant --- PROPN --- True --- 2732174988197022273 --- False\n87\nHernandez -- Hernandez --- PROPN --- True --- 5312260791442479864 --- False\n"
     ]
    }
   ],
   "source": [
    "# set up NLP\n",
    "nlp_0 = spacy.load('en_core_web_sm')\n",
    "doc_test_0 = nlp_0(test_text)\n",
    "\n",
    "for i,token in enumerate(doc_test_0):\n",
    "    print(i)\n",
    "    print(token.text,'--',token.lemma_,'---',token.pos_,'---',token.has_vector,'---',nlp_0.vocab.strings[str(token)], '---',token.is_stop)"
   ]
  },
  {
   "source": [
    "### Strategy 1: Pre-Tokenization Custom Filtering Pipe (Tamar's V1)\n",
    "NOTES: \n",
    "- removing punctiation/stop works/keeps POS tagging from working properly\n",
    "- Lets explore post processing teqniques"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['tok2vec', 'stopwords_1', 'punctuation_1', 'repeats_1', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "nlp_1 = spacy.load('en_core_web_sm')\n",
    "\n",
    "@Language.component(\"repeats_1\")\n",
    "def repeats_1(doc):\n",
    "  s = doc.text.lower()\n",
    "  i = (s+\" \"+s).find(s, 1, -1)\n",
    "  if i == -1:\n",
    "    doc = doc\n",
    "  else:\n",
    "    doc = nlp_1.make_doc(s[:i-1])\n",
    "  return(doc)\n",
    "\n",
    "# component for removing stop words\n",
    "@Language.component(\"stopwords_1\")\n",
    "def stopwords_1(doc):\n",
    "  doc = [t.text for t in doc if not t.is_stop]\n",
    "  doc = nlp_1.make_doc(' '.join(map(str, doc)))\n",
    "  return(doc)\n",
    "\n",
    "# component for removing punctuation\n",
    "@Language.component(\"punctuation_1\")\n",
    "def punctuation_1(doc):\n",
    "  doc = [t.text for t in doc if (not t.is_punct and not t.is_space)]\n",
    "  doc = nlp_1.make_doc(' '.join(map(str, doc)))\n",
    "  return(doc)\n",
    "\n",
    "\n",
    "nlp_1.add_pipe(\"stopwords_1\", name = \"stopwords_1\", before = \"tagger\") # add stopword remover to pipeline\n",
    "# add punctuation remover to pipeline\n",
    "nlp_1.add_pipe(\"punctuation_1\", name = \"punctuation_1\", before = \"tagger\")\n",
    "# add repeats remover to pipeline\n",
    "nlp_1.add_pipe(\"repeats_1\", name = \"repeats_1\", before = \"tagger\")\n",
    "# nlp.add_pipe(\"contextual spellchecker\")\n",
    "\n",
    "doc_test_1 = nlp_1(test_text)\n",
    "doc_test_1\n",
    "\n",
    "print(nlp_1.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\nreported -- reported --- ADP --- False --- 11181246799942687462 --- False\n1\naccused -- accused --- ADP --- False --- 8915410849535181575 --- False\n2\nofficer -- officer --- ADP --- False --- 9228201189916158328 --- False\n3\nfailed -- failed --- ADP --- False --- 4500079622559289248 --- False\n4\nterminate -- terminate --- ADP --- False --- 974796105764162566 --- False\n5\nmotor -- motor --- ADP --- False --- 1640505308719491870 --- False\n6\nvehicle -- vehicle --- ADP --- False --- 854351138829791262 --- False\n7\npursuit -- pursuit --- ADP --- False --- 14575054513208559317 --- False\n8\nordered -- ordered --- ADP --- False --- 18198004002626200087 --- False\n9\nsergeant -- sergeant --- ADP --- False --- 10122840340774289451 --- False\n10\nhernandez -- hernandez --- ADP --- False --- 5211270583463062168 --- False\n"
     ]
    }
   ],
   "source": [
    "for i,token in enumerate(doc_test_1):\n",
    "    print(i)\n",
    "    print(token.text,'--',token.lemma_,'---',token.pos_,'---',token.has_vector,'---',nlp_1.vocab.strings[str(token)], '---',token.is_stop)"
   ]
  },
  {
   "source": [
    "### Strategy 2: Filtering Docs After Tagging\n",
    "NOTES:\n",
    "- Filters are effectively applied\n",
    "- Tokens loose important attributes (pos/lemma/vector)\n",
    "- Token still maintain hash-ids"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer', 'stopwords_2', 'punctuation_2', 'repeats_2']\n"
     ]
    }
   ],
   "source": [
    "# set up strategy 2 NLP\n",
    "nlp_2 = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "@Language.component(\"repeats_2\")\n",
    "def repeats_2(doc):\n",
    "  s = doc.text.lower()\n",
    "  i = (s+\" \"+s).find(s, 1, -1)\n",
    "  if i == -1:\n",
    "    doc = doc\n",
    "  else:\n",
    "    doc = nlp_2.make_doc(s[:i-1])\n",
    "  return(doc)\n",
    "\n",
    "# component for removing stop words\n",
    "@Language.component(\"stopwords_2\")\n",
    "def stopwords_2(doc):\n",
    "  doc = [t.text for t in doc if not t.is_stop]\n",
    "  doc = nlp_2.make_doc(' '.join(map(str, doc)))\n",
    "  return(doc)\n",
    "\n",
    "# component for removing punctuation\n",
    "@Language.component(\"punctuation_2\")\n",
    "def punctuation_2(doc):\n",
    "  doc = [t.text for t in doc if (not t.is_punct and not t.is_space)]\n",
    "  doc = nlp_2.make_doc(' '.join(map(str, doc)))\n",
    "  return(doc)\n",
    "\n",
    "#add sustom pipes\n",
    "nlp_2.add_pipe(\"stopwords_2\", name = \"stopwords_2\", last = True) # add stopword remover to pipeline\n",
    "# add punctuation remover to pipeline\n",
    "nlp_2.add_pipe(\"punctuation_2\", name = \"punctuation_2\", last = True)\n",
    "# add repeats remover to pipeline\n",
    "nlp_2.add_pipe(\"repeats_2\", name = \"repeats_2\", last = True)\n",
    "# nlp.add_pipe(\"contextual spellchecker\")\n",
    "\n",
    "print(nlp_2.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "reported accused officer failed terminate motor vehicle pursuit ordered sergeant hernandez"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "doc_test_2 = nlp_2(test_text)\n",
    "doc_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\nreported --  ---  --- False --- 11181246799942687462 --- False\n1\naccused --  ---  --- False --- 8915410849535181575 --- False\n2\nofficer --  ---  --- False --- 9228201189916158328 --- False\n3\nfailed --  ---  --- False --- 4500079622559289248 --- False\n4\nterminate --  ---  --- False --- 974796105764162566 --- False\n5\nmotor --  ---  --- False --- 1640505308719491870 --- False\n6\nvehicle --  ---  --- False --- 854351138829791262 --- False\n7\npursuit --  ---  --- False --- 14575054513208559317 --- False\n8\nordered --  ---  --- False --- 18198004002626200087 --- False\n9\nsergeant --  ---  --- False --- 10122840340774289451 --- False\n10\nhernandez --  ---  --- False --- 5211270583463062168 --- False\n"
     ]
    }
   ],
   "source": [
    "for i,token in enumerate(doc_test_2):\n",
    "    print(i)\n",
    "    print(token.text,'--',token.lemma_,'---',token.pos_,'---',token.has_vector,'---',nlp_2.vocab.strings[str(token)], '---',token.is_stop)"
   ]
  },
  {
   "source": [
    "### Strategy 3: Filtering Docs After Tagging\n",
    "NOTES:\n",
    "- Filters are ineffectively applied\n",
    "- Tokens loose important attributes (pos/lemma/vector)\n",
    "- Token still maintain hash-ids\n",
    "- Cycled though each step in the pipeline and only works at \"after = lemmatizer\". syspeciously doesn't produce the same result at just the default \"last = True\"\n",
    "- My understanding of spacy says this should work...."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer', 'repeats_3', 'punctuation_3', 'stopwords_3']\n"
     ]
    }
   ],
   "source": [
    "# set up strategy 3 NLP\n",
    "nlp_3 = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "@Language.component(\"repeats_3\")\n",
    "def repeats_3(doc):\n",
    "  s = doc.text.lower()\n",
    "  i = (s+\" \"+s).find(s, 1, -1)\n",
    "  if i == -1:\n",
    "    doc = doc\n",
    "  else:\n",
    "    doc = nlp_3.make_doc(s[:i-1])\n",
    "  return(doc)\n",
    "\n",
    "# component for removing stop words\n",
    "@Language.component(\"stopwords_3\")\n",
    "def stopwords_3(doc):\n",
    "  doc = [t.text for t in doc if not t.is_stop]\n",
    "  doc = nlp_3.make_doc(' '.join(map(str, doc)))\n",
    "  return(doc)\n",
    "\n",
    "# component for removing punctuation\n",
    "@Language.component(\"punctuation_3\")\n",
    "def punctuation_3(doc):\n",
    "  doc = [t.text for t in doc if (not t.is_punct and not t.is_space)]\n",
    "  doc = nlp_3.make_doc(' '.join(map(str, doc)))\n",
    "  return(doc)\n",
    "\n",
    "#add custom pipes\n",
    "nlp_3.add_pipe(\"stopwords_3\", name = \"stopwords_3\", after = 'lemmatizer') # add stopword remover to pipeline\n",
    "# add punctuation remover to pipeline\n",
    "nlp_3.add_pipe(\"punctuation_3\", name = \"punctuation_3\", after = 'lemmatizer')\n",
    "# add repeats remover to pipeline\n",
    "nlp_3.add_pipe(\"repeats_3\", name = \"repeats_3\", after = 'lemmatizer')\n",
    "# nlp.add_pipe(\"contextual spellchecker\")\n",
    "\n",
    "print(nlp_3.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\nreported --  ---  --- False --- 11181246799942687462 --- False\n1\naccused --  ---  --- False --- 8915410849535181575 --- False\n2\nofficer --  ---  --- False --- 9228201189916158328 --- False\n3\nfailed --  ---  --- False --- 4500079622559289248 --- False\n4\nterminate --  ---  --- False --- 974796105764162566 --- False\n5\nmotor --  ---  --- False --- 1640505308719491870 --- False\n6\nvehicle --  ---  --- False --- 854351138829791262 --- False\n7\npursuit --  ---  --- False --- 14575054513208559317 --- False\n8\nordered --  ---  --- False --- 18198004002626200087 --- False\n9\nSergeant --  ---  --- False --- 2732174988197022273 --- False\n10\nHernandez --  ---  --- False --- 5312260791442479864 --- False\n11\nreported --  ---  --- False --- 11181246799942687462 --- False\n12\naccused --  ---  --- False --- 8915410849535181575 --- False\n13\nofficer --  ---  --- False --- 9228201189916158328 --- False\n14\nfailed --  ---  --- False --- 4500079622559289248 --- False\n15\nterminate --  ---  --- False --- 974796105764162566 --- False\n16\nmotor --  ---  --- False --- 1640505308719491870 --- False\n17\nvehicle --  ---  --- False --- 854351138829791262 --- False\n18\npursuit --  ---  --- False --- 14575054513208559317 --- False\n19\nordered --  ---  --- False --- 18198004002626200087 --- False\n20\nSergeant --  ---  --- False --- 2732174988197022273 --- False\n21\nHernandez --  ---  --- False --- 5312260791442479864 --- False\n22\nreported --  ---  --- False --- 11181246799942687462 --- False\n23\naccused --  ---  --- False --- 8915410849535181575 --- False\n24\nofficer --  ---  --- False --- 9228201189916158328 --- False\n25\nfailed --  ---  --- False --- 4500079622559289248 --- False\n26\nterminate --  ---  --- False --- 974796105764162566 --- False\n27\nmotor --  ---  --- False --- 1640505308719491870 --- False\n28\nvehicle --  ---  --- False --- 854351138829791262 --- False\n29\npursuit --  ---  --- False --- 14575054513208559317 --- False\n30\nordered --  ---  --- False --- 18198004002626200087 --- False\n31\nsergeant --  ---  --- False --- 10122840340774289451 --- False\n32\nHernandez --  ---  --- False --- 5312260791442479864 --- False\n33\nreported --  ---  --- False --- 11181246799942687462 --- False\n34\naccused --  ---  --- False --- 8915410849535181575 --- False\n35\nofficer --  ---  --- False --- 9228201189916158328 --- False\n36\nfailed --  ---  --- False --- 4500079622559289248 --- False\n37\nterminate --  ---  --- False --- 974796105764162566 --- False\n38\nmotor --  ---  --- False --- 1640505308719491870 --- False\n39\nvehicle --  ---  --- False --- 854351138829791262 --- False\n40\npursuit --  ---  --- False --- 14575054513208559317 --- False\n41\nordered --  ---  --- False --- 18198004002626200087 --- False\n42\nSergeant --  ---  --- False --- 2732174988197022273 --- False\n43\nHernandez --  ---  --- False --- 5312260791442479864 --- False\n"
     ]
    }
   ],
   "source": [
    "doc_test_3 = nlp_3(test_text)\n",
    "\n",
    "for i,token in enumerate(doc_test_3):\n",
    "    print(i)\n",
    "    print(token.text,'--',token.lemma_,'---',token.pos_,'---',token.has_vector,'---',nlp_2.vocab.strings[str(token)], '---',token.is_stop)"
   ]
  },
  {
   "source": [
    "### Strategy 4: Create list of Tokens w/ Preserved Attributes\n",
    "NOTES:\n",
    "- List of token maintains important token attributes\n",
    "- fitleres by stop words, punctuation, repeats.\n",
    "- Filters are ineffectively applied\n",
    "- \"repeated\" elements are those with identical .text attributes and .pos_ attributes.  The idea is to avoid conflating identical words with different POS tags.  Can potentially update this to include other differentating features if it comes up.\n",
    "- Over all good strategy for leveraging spacy's speech tagging and the going with a bag of words style model\n",
    "- Potential imporvements:\n",
    "     - Include custom word count attribute (count the number of occurances for a specific repeat)\n",
    "     - Include word position information (? from origional doc or from filteres doc? lot to sort out, interesting to think about)\n",
    "     - funky stuff happening with capitalizations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[reported,\n",
       " accused,\n",
       " officer,\n",
       " failed,\n",
       " terminate,\n",
       " motor,\n",
       " vehicle,\n",
       " pursuit,\n",
       " ordered,\n",
       " Sergeant,\n",
       " Hernandez]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# set up NLP\n",
    "nlp_4 = spacy.load('en_core_web_sm')\n",
    "doc_test_4 = nlp_4(test_text)\n",
    "tok_list_working = [t for t in doc_test_4 if not t.is_stop]\n",
    "tok_list_working = [t for t in tok_list_working if (not t.is_punct and not t.is_space)]\n",
    "\n",
    "# tok_list_working\n",
    "count_dict = {}\n",
    "for i, tok in enumerate(tok_list_working):\n",
    "    count_dict[tok.text.lower() + \"_\" + tok.pos_] = []\n",
    "for i, tok in enumerate(tok_list_working):\n",
    "    count_dict[tok.text.lower() + \"_\" + tok.pos_].append(i)\n",
    "\n",
    "unique_index = []\n",
    "for element in count_dict:\n",
    "    unique_index.append(count_dict[element][0])\n",
    "\n",
    "tok_list_final = [tok_list_working[i] for i in unique_index]\n",
    "tok_list_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\nreported -- report --- VERB --- True --- 11181246799942687462 --- False\n1\naccused -- accuse --- VERB --- True --- 8915410849535181575 --- False\n2\nofficer -- officer --- NOUN --- True --- 9228201189916158328 --- False\n3\nfailed -- fail --- VERB --- True --- 4500079622559289248 --- False\n4\nterminate -- terminate --- VERB --- True --- 974796105764162566 --- False\n5\nmotor -- motor --- NOUN --- True --- 1640505308719491870 --- False\n6\nvehicle -- vehicle --- NOUN --- True --- 854351138829791262 --- False\n7\npursuit -- pursuit --- NOUN --- True --- 14575054513208559317 --- False\n8\nordered -- order --- VERB --- True --- 18198004002626200087 --- False\n9\nSergeant -- Sergeant --- PROPN --- True --- 2732174988197022273 --- False\n10\nHernandez -- Hernandez --- PROPN --- True --- 5312260791442479864 --- False\n"
     ]
    }
   ],
   "source": [
    "for i,token in enumerate(tok_list_final):\n",
    "    print(i)\n",
    "    print(token.text,'--',token.lemma_,'---',token.pos_,'---',token.has_vector,'---',nlp_0.vocab.strings[str(token)], '---',token.is_stop)"
   ]
  },
  {
   "source": [
    "### Strategy 5: Explicity Filter and Reconstruct Doc object\n",
    "NOTES:\n",
    "- Filters are ineffectively applied\n",
    "- Tokens vector for some reason aren't there any more\n",
    "- Tokens maintain other important attributes: POS, lemma, etc etc\n",
    "- Token still maintain hash-ids\n",
    "- Maybe come up with a way to add element count as a custom attribute to the word in the doc?\n",
    "- Basically we exploit the class structure of the token/doc objects to make a frankenstein doc.  not sure home much sense this makes for application but w/e."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tokens(doc, index_to_del, list_attr=[LOWER, POS, ENT_TYPE, IS_ALPHA, DEP, LEMMA, LOWER, IS_PUNCT, IS_DIGIT, IS_SPACE, IS_STOP]):\n",
    "    \"\"\"\n",
    "    Remove tokens from a Spacy *Doc* object without losing \n",
    "    associated information (PartOfSpeech, Dependance, Lemma, extensions, ...)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : spacy.tokens.doc.Doc\n",
    "        spacy representation of the text\n",
    "    index_to_del : list of integer \n",
    "         positions of each token you want to delete from the document\n",
    "    list_attr : list, optional\n",
    "        Contains the Spacy attributes you want to keep (the default is \n",
    "        [LOWER, POS, ENT_TYPE, IS_ALPHA, DEP, LEMMA, LOWER, IS_PUNCT, IS_DIGIT, IS_SPACE, IS_STOP])\n",
    "    Returns\n",
    "    -------\n",
    "    spacy.tokens.doc.Doc\n",
    "        Filtered version of doc\n",
    "    \"\"\"\n",
    "    \n",
    "    np_array = doc.to_array(list_attr) # Array representation of Doc\n",
    "    \n",
    "    # Creating a mask: boolean array of the indexes to delete\n",
    "    mask_to_del = np.ones(len(np_array), np.bool)\n",
    "    mask_to_del[index_to_del] = 0\n",
    "    \n",
    "    np_array_2 = np_array[mask_to_del]\n",
    "    doc2 = Doc(doc.vocab, words=[t.text for t in doc if t.i not in index_to_del])\n",
    "    doc2.from_array(list_attr, np_array_2)\n",
    "    \n",
    "    ### Modification made by @yarongon https://gist.github.com/Jacobe2169/5086c7c4f6c56e9d3c7cfb1eb0010fe8#gistcomment-2941380\n",
    "    # Handling user extensions\n",
    "    #  The `doc.user_data` dictionary is holding the data backing user-defined attributes.\n",
    "    #  The data is based on characters offset, so a conversion is needed from the\n",
    "    #  old Doc to the new one.\n",
    "    #  More info here: https://github.com/explosion/spaCy/issues/2532\n",
    "    arr = np.arange(len(doc))\n",
    "    new_index_to_old = arr[mask_to_del]\n",
    "    doc_offset_2_token = {tok.idx : tok.i  for tok in doc}  # needed for the user data\n",
    "    doc2_token_2_offset = {tok.i : tok.idx  for tok in doc2}  # needed for the user data\n",
    "    new_user_data = {}\n",
    "    for ((prefix, ext_name, offset, x), val) in doc.user_data.items():\n",
    "        old_token_index = doc_offset_2_token[offset]\n",
    "        new_token_index = np.where(new_index_to_old == old_token_index)[0]\n",
    "        if new_token_index.size == 0:  # Case this index was deleted\n",
    "            continue\n",
    "        new_char_index = doc2_token_2_offset[new_token_index[0]]\n",
    "        new_user_data[(prefix, ext_name, new_char_index, x)] = val\n",
    "    doc2.user_data = new_user_data\n",
    "    \n",
    "    return doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "reported accused officer failed terminate motor vehicle pursuit ordered Sergeant Hernandez \n"
     ]
    }
   ],
   "source": [
    "nlp_5 = spacy.load('en_core_web_sm')\n",
    "doc_test_5 = nlp_5(test_text)\n",
    "\n",
    "#collect stop word/punctuation indices \n",
    "index_list = []\n",
    "for i, tok in enumerate(doc_test_5):\n",
    "    if tok.is_stop:\n",
    "        index_list.append(i)\n",
    "    if (tok.is_punct or tok.is_space):\n",
    "        index_list.append(i)\n",
    "\n",
    "#Collect repeated element indices\n",
    "count_dict = {}\n",
    "\n",
    "for i, tok in enumerate(doc_test_5):\n",
    "    count_dict[tok.text.lower()] = []\n",
    "for i, tok in enumerate(doc_test_5):\n",
    "    count_dict[tok.text.lower()].append(i)\n",
    "\n",
    "repeat_indeces = []\n",
    "for element in count_dict:\n",
    "    repeat_indeces +=count_dict[element][1:]\n",
    "\n",
    "#combine unwanted indices\n",
    "final_indeces = repeat_indeces + index_list\n",
    "final_indeces\n",
    "\n",
    "doc_test_5 = remove_tokens(doc_test_5,index_to_del=final_indeces)\n",
    "print(doc_test_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\nreported -- report --- VERB --- False --- 11181246799942687462 --- False\n1\naccused -- accuse --- VERB --- False --- 8915410849535181575 --- False\n2\nofficer -- officer --- NOUN --- False --- 9228201189916158328 --- False\n3\nfailed -- fail --- VERB --- False --- 4500079622559289248 --- False\n4\nterminate -- terminate --- VERB --- False --- 974796105764162566 --- False\n5\nmotor -- motor --- NOUN --- False --- 1640505308719491870 --- False\n6\nvehicle -- vehicle --- NOUN --- False --- 854351138829791262 --- False\n7\npursuit -- pursuit --- NOUN --- False --- 14575054513208559317 --- False\n8\nordered -- order --- VERB --- False --- 18198004002626200087 --- False\n9\nSergeant -- Sergeant --- PROPN --- False --- 2732174988197022273 --- False\n10\nHernandez -- Hernandez --- PROPN --- False --- 5312260791442479864 --- False\n"
     ]
    }
   ],
   "source": [
    "for i,token in enumerate(doc_test_5):\n",
    "    print(i)\n",
    "    print(token.text,'--',token.lemma_,'---',token.pos_,'---',token.has_vector,'---',nlp_2.vocab.strings[str(token)], '---',token.is_stop)"
   ]
  },
  {
   "source": [
    "### Strategy 6: Add Custom Attributes Tagger\n",
    "NOTES: note attempted"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}